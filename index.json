{
  "articles/benchmark-1.0.0.html": {
    "href": "articles/benchmark-1.0.0.html",
    "title": "| Cursively",
    "keywords": "This benchmark tests the simple act of counting how many records are in a CSV file. It's not a simple count of how many lines are in the text file: line breaks within quoted fields must be treated as data, and multiple line breaks in a row must be treated as one, since each record must have at least one field. Therefore, assuming correct implementations, this benchmark should test the raw CSV processing speed. Cursively eliminates a ton of overhead found in libraries such as CsvHelper by restricting the allowed input encodings and using the visitor pattern as its only means of output. Cursively can scan through the original bytes of the input to do its work, and it can give slices of the input data directly to the consumer without having to copy or allocate. Therefore, these benchmarks are somewhat biased in favor of Cursively, as CsvHelper relies on external code to transform the data to UTF-16. This isn't as unfair as that makes it sound: the overwhelming majority of input files are probably UTF-8 anyway (or a compatible SBCS), so this transformation is something that practically every user will experience. Input files can be found here: https://github.com/airbreather/Cursively/tree/v1.0.0/test/Cursively.Benchmark/large-csv-files Benchmark source code is a slightly edited* version of this: https://github.com/airbreather/Cursively/tree/v1.0.0/test/Cursively.Benchmark *edited only to remove CoreRtJob and the more-or-less redundant NopUsingCursively Raw BenchmarkDotNet output is at the bottom, but here are some numbers derived from it. The data was fully loaded in main memory when running these tests. This summary also does not indicate anything about the GC pressure: CSV File Runtime Library Throughput 100 records / 10,000 tiny fields each .NET 4.7.2 Cursively 99.81 MiB/s 100 records / 10,000 tiny fields each .NET 4.7.2 CsvHelper 22.60 MiB/s 100 records / 10,000 tiny fields each .NET Core 2.2.5 Cursively 126.1 MiB/s 100 records / 10,000 tiny fields each .NET Core 2.2.5 CsvHelper 25.32 MiB/s 100 records / 10,000 tiny quoted fields each .NET 4.7.2 Cursively 118.5 MiB/s 100 records / 10,000 tiny quoted fields each .NET 4.7.2 CsvHelper 25.05 MiB/s 100 records / 10,000 tiny quoted fields each .NET Core 2.2.5 Cursively 187.0 MiB/s 100 records / 10,000 tiny quoted fields each .NET Core 2.2.5 CsvHelper 27.96 MiB/s 10,000 records / 1,000 empty fields each .NET 4.7.2 Cursively 64.15 MiB/s 10,000 records / 1,000 empty fields each .NET 4.7.2 CsvHelper 15.57 MiB/s 10,000 records / 1,000 empty fields each .NET Core 2.2.5 Cursively 112.7 MiB/s 10,000 records / 1,000 empty fields each .NET Core 2.2.5 CsvHelper 14.84 MiB/s Mock data from Mockaroo .NET 4.7.2 Cursively 1.637 GiB/s Mock data from Mockaroo .NET 4.7.2 CsvHelper 74.81 MiB/s Mock data from Mockaroo .NET Core 2.2.5 Cursively 1.893 GiB/s Mock data from Mockaroo .NET Core 2.2.5 CsvHelper 66.86 MiB/s Raw BenchmarkDotNet output: BenchmarkDotNet=v0.11.5, OS=Windows 10.0.17134.765 (1803/April2018Update/Redstone4) Intel Core i7-6850K CPU 3.60GHz (Skylake), 1 CPU, 12 logical and 6 physical cores Frequency=3515622 Hz, Resolution=284.4447 ns, Timer=TSC .NET Core SDK=2.2.300 [Host] : .NET Core 2.2.5 (CoreCLR 4.6.27617.05, CoreFX 4.6.27618.01), 64bit RyuJIT Job-ASLTDW : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3416.0 Job-RICADF : .NET Core 2.2.5 (CoreCLR 4.6.27617.05, CoreFX 4.6.27618.01), 64bit RyuJIT Server=True Method Runtime csvFile Mean Error StdDev Ratio RatioSD Gen 0 Gen 1 Gen 2 Allocated CountRowsUsingCursively Clr 100-huge-records 27.714 ms 0.0126 ms 0.0105 ms 1.00 0.00 - - - 256 B CountRowsUsingCsvHelper Clr 100-huge-records 122.397 ms 0.1685 ms 0.1494 ms 4.42 0.01 17250.0000 6250.0000 750.0000 110257334 B CountRowsUsingCursively Core 100-huge-records 21.932 ms 0.0254 ms 0.0226 ms 1.00 0.00 - - - 56 B CountRowsUsingCsvHelper Core 100-huge-records 109.261 ms 0.3319 ms 0.3104 ms 4.98 0.02 400.0000 200.0000 - 110256320 B CountRowsUsingCursively Clr 100-h(...)uoted [23] 39.453 ms 0.0974 ms 0.0864 ms 1.00 0.00 - - - 683 B CountRowsUsingCsvHelper Clr 100-h(...)uoted [23] 186.572 ms 0.4682 ms 0.4380 ms 4.73 0.01 24666.6667 9666.6667 666.6667 153595995 B CountRowsUsingCursively Core 100-h(...)uoted [23] 24.995 ms 0.0160 ms 0.0142 ms 1.00 0.00 - - - 56 B CountRowsUsingCsvHelper Core 100-h(...)uoted [23] 167.160 ms 0.3437 ms 0.3215 ms 6.69 0.02 333.3333 - - 153579848 B CountRowsUsingCursively Clr 10k-empty-records 148.952 ms 0.2502 ms 0.2340 ms 1.00 0.00 - - - 2048 B CountRowsUsingCsvHelper Clr 10k-empty-records 613.718 ms 0.8869 ms 0.7862 ms 4.12 0.01 66000.0000 2000.0000 - 420838944 B CountRowsUsingCursively Core 10k-empty-records 84.801 ms 0.1079 ms 0.1009 ms 1.00 0.00 - - - 56 B CountRowsUsingCsvHelper Core 10k-empty-records 644.051 ms 2.8782 ms 2.5515 ms 7.60 0.03 2000.0000 - - 420832856 B CountRowsUsingCursively Clr mocked 7.242 ms 0.0233 ms 0.0207 ms 1.00 0.00 - - - 64 B CountRowsUsingCsvHelper Clr mocked 162.298 ms 0.2958 ms 0.2622 ms 22.41 0.08 18000.0000 333.3333 - 115764389 B CountRowsUsingCursively Core mocked 6.264 ms 0.0115 ms 0.0107 ms 1.00 0.00 - - - 56 B CountRowsUsingCsvHelper Core mocked 181.592 ms 0.3413 ms 0.3193 ms 28.99 0.09 333.3333 - - 115757736 B"
  },
  "release-notes.html": {
    "href": "release-notes.html",
    "title": "Cursively Release Notes | Cursively",
    "keywords": "Cursively Release Notes 1.0.0 Initial release."
  },
  "index.html": {
    "href": "index.html",
    "title": "Cursively | Cursively",
    "keywords": "Cursively A fast, RFC 4180 -conforming CSV reading library for .NET. Written in C#. Fully supports all UTF-8 encoded byte streams. Other encodings will work as well, as long as the bytes 0x0A , 0x0D , 0x22 , and 0x2C are all guaranteed to mean the same thing that they mean in ASCII / UTF-8, and as long as the encoding defines no other byte sequences which identify the Unicode code points for '\\n' , '\\r' , '\"' , or ',' , respectively. In practice, this means that most \"Extended ASCII\" code pages will probably work, probably including all SBCS. Many \"Extended ASCII\" DBCS will probably work too, but it looks like Shift-JIS will not work. Notably, this library will fail to yield the correct result when used with byte streams encoded in any variant of UTF-16 or UTF-32, even with a BOM header. If you require that support, there are other libraries that should work for you. Fully supports all streams that completely conform to the RFC 4180 format, and defines rules for how to handle streams that break certain rules of RFC 4180 in a way that seems to be consistent with other popular tools, at a minor speed penalty. This library exists because the original developer was unsatisfied with the performance characteristics of raw CSV processing tools. Everything out there seemed to have some combination of these flaws: Tons of managed heap allocations on hot paths, often baked into the API requirements Decoding to UTF-16LE before scanning for critical bytes, which could be considered a subset of: The design forces a ton of processing to happen on the input which the caller might not even care about Omitting important parts of RFC 4180 Disappointing options for mitigating DDoS risk \"RFC 4180 over UTF-8\" is a very simple byte stream format, and the state machine requires only a few extra states to define how to handle all UTF-8 streams that are non-RFC 4180, so it seemed odd that there wasn't a reader without these flaws. With Cursively, each stream only strictly requires a grand total of two objects to be allocated on the managed heap*, *in case this is too much, both could be reset and put into a pool to be reused for processing other streams processing happens directly on the input bytes (no decoding is done by Cursively itself), the only processing that Cursively necessarily does is the bare minimum needed to describe the data to the caller, inputs that conform to RFC 4180* are processed according to all the rules of RFC 4180, and *inputs that do not conform to RFC 4180 are handled according to consistent, intuitive rules there is a very low risk* of DDoS directly from using Cursively, and the caller has the tools that they need in order to prevent (or respond to) attacks in a more \"natural\" way than other CSV libraries that the developer has seen. *There is no such thing as \"risk-free\" in our world. Cursively itself cannot eliminate the risk of attacks that use it as a vector to exploit defects in CoreFX / C# compiler / runtime / OS / hardware. Future enhancements may add support for byte streams in other encodings if there's demand for it, but not at the expense of anything that matters to the \"RFC 4180 over UTF-8\" use case."
  },
  "api/Cursively.html": {
    "href": "api/Cursively.html",
    "title": "Namespace Cursively | Cursively",
    "keywords": "Namespace Cursively Classes CsvReaderVisitorBase Base class for listeners that process a stream of RFC 4180 (CSV) tokens from an instance of CsvTokenizer . CsvTokenizer Tokenizes a byte stream into CSV fields. The processing follows the guidelines set out in RFC 4180 unless and until the stream proves to be in an incompatible format, in which case a set of additional rules kick in to ensure that all streams are still compatible. The byte stream is tokenized according to the rules of the ASCII encoding. This makes it compatible with any encoding that encodes 0x0A, 0x0D, 0x22, and 0x2C the same way that ASCII encodes them. UTF-8 and Extended ASCII SBCS are notable examples of acceptable encodings. UTF-16 is a notable example of an unacceptable encoding; trying to use this class to process text encoded in an unacceptable encoding will yield undesirable results without any errors. All bytes that appear in the stream except 0x0A, 0x0D, 0x22, and 0x2C are unconditionally treated as data and passed through as-is. It is the consumer's responsibility to handle (or not handle) NUL bytes, invalid UTF-8, leading UTF-8 BOM, or any other quirks that come with the territory of text processing."
  },
  "api/Cursively.CsvTokenizer.html": {
    "href": "api/Cursively.CsvTokenizer.html",
    "title": "Class CsvTokenizer | Cursively",
    "keywords": "Class CsvTokenizer Tokenizes a byte stream into CSV fields. The processing follows the guidelines set out in RFC 4180 unless and until the stream proves to be in an incompatible format, in which case a set of additional rules kick in to ensure that all streams are still compatible. The byte stream is tokenized according to the rules of the ASCII encoding. This makes it compatible with any encoding that encodes 0x0A, 0x0D, 0x22, and 0x2C the same way that ASCII encodes them. UTF-8 and Extended ASCII SBCS are notable examples of acceptable encodings. UTF-16 is a notable example of an unacceptable encoding; trying to use this class to process text encoded in an unacceptable encoding will yield undesirable results without any errors. All bytes that appear in the stream except 0x0A, 0x0D, 0x22, and 0x2C are unconditionally treated as data and passed through as-is. It is the consumer's responsibility to handle (or not handle) NUL bytes, invalid UTF-8, leading UTF-8 BOM, or any other quirks that come with the territory of text processing. Inheritance Object CsvTokenizer Inherited Members Object.Equals(Object) Object.Equals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Object.ReferenceEquals(Object, Object) Object.ToString() Namespace : Cursively Assembly : Cursively.dll Syntax public class CsvTokenizer Remarks Each instance of this class expects to process all data from one stream, represented as zero or more ProcessNextChunk(ReadOnlySpan<Byte>, CsvReaderVisitorBase) followed by one ProcessEndOfStream(CsvReaderVisitorBase) , before moving on to another stream. An instance may be reused after a stream has been fully processed, but each instance is also very lightweight, so it is recommended that callers simply create a new instance for each stream that needs to be processed. RFC 4180 leaves a lot of wiggle room for implementers. The following section explains how this implementation resolves ambiguities in the spec, explains where and why we deviate from it, and offers clarifying notes where the spec appears to have \"gotchas\", in the order that the relevant items appear in the spec, primarily modeled off of how Josh Close's CsvHelper library handles the same situations: The spec says that separate lines are delimited by CRLF line breaks. This implementation accepts line breaks of any format (CRLF, LF, CR). The spec says that there may or may not be a line break at the end of the last record in the stream. This implementation does not require there to be a line break, and it would not hurt to add one either. The spec refers to an optional header line at the beginning. This implementation does not include any special treatment for the first line of fields; if they need to be treated as headers, then the consumer needs to know that and respond accordingly. The spec says each record may contain \"one or more fields\". This implementation interprets that to mean strictly that any number of consecutive newline characters in a row are treated as one. Many implementations allow the delimiter character to be configured to be something else other than a comma. This implementation does not currently offer that flexibility. Many implementations allow automatically trimming whitespace at the beginning and/or end of each field (sometimes optionally). The spec expressly advises against doing that, and this implementation follows suit. It is our opinion that consumers ought to be more than capable of trimming spaces at the beginning or end as part of their processing if this is desired. The spec says that the last field in a record must not be followed by a comma. This implementation interprets that to mean that if we do see a comma followed immediately by a line ending character, then that represents the data for an empty field. Finally, the spec has a lot to say about double quotes. This implementation follows the rules that it expressly lays out, but there are some \"gotchas\" that follow from the spec leaving it open-ended how implementations should deal with various streams that include double quotes which do not completely enclose fields, resolved as follows: If a double quote is encountered at the very beginning of a field, then all characters up until the next unescaped double quote or the end of the stream (whichever comes first) are considered to be part of the data for that field (we do translate escaped double quotes for convenience). This includes line ending characters, even though Excel seems to only make that happen if the field counts matching up. If parsing stopped at an unescaped double quote, but there are still more bytes after that double quote before the next delimiter, then all those bytes will be treated verbatim as part of the field's data (double quotes are no longer special at all for the remainder of the field). Double quotes encountered at any other point are included verbatim as part of the field with no special processing. var visitor = new MyVisitorSubclass(); var tokenizer = new CsvTokenizer(); tokenizer.ProcessNextChunk(File.ReadAllBytes(\"...\"), visitor); tokenizer.ProcessEndOfStream(visitor); using (var stream = File.OpenRead(\"...\")) { var visitor = new MyVisitorSubclass(); var tokenizer = new CsvTokenizer(); var buffer = new byte[81920]; int lastRead; while ((lastRead = stream.Read(buffer, 0, buffer.Length)) != 0) { tokenizer.ProcessNextChunk(new ReadOnlySpan<byte>(buffer, 0, lastRead), visitor); } tokenizer.ProcessEndOfStream(visitor); } Methods | Improve this Doc View Source ProcessEndOfStream(CsvReaderVisitorBase) Informs this tokenizer that the last chunk of data in the stream has been read, and so we should make any final interactions with the CsvReaderVisitorBase and reset our state to prepare for the next stream. Declaration public void ProcessEndOfStream(CsvReaderVisitorBase visitor) Parameters Type Name Description CsvReaderVisitorBase visitor The CsvReaderVisitorBase to interact with, or null if we should simply advance the parser state. Remarks If ProcessNextChunk(ReadOnlySpan<Byte>, CsvReaderVisitorBase) has never been called (or has not been called since the last time that this method was called), then this method will do nothing. | Improve this Doc View Source ProcessNextChunk(ReadOnlySpan<Byte>, CsvReaderVisitorBase) Accepts the next (or first) chunk of data in the CSV stream, and informs an instance of CsvReaderVisitorBase what it contains. Declaration public void ProcessNextChunk(ReadOnlySpan<byte> chunk, CsvReaderVisitorBase visitor) Parameters Type Name Description ReadOnlySpan < Byte > chunk A ReadOnlySpan<T> containing the next chunk of data. CsvReaderVisitorBase visitor The CsvReaderVisitorBase to interact with, or null if we should simply advance the parser state. Remarks If chunk is empty, this method will do nothing."
  },
  "api/Cursively.CsvReaderVisitorBase.html": {
    "href": "api/Cursively.CsvReaderVisitorBase.html",
    "title": "Class CsvReaderVisitorBase | Cursively",
    "keywords": "Class CsvReaderVisitorBase Base class for listeners that process a stream of RFC 4180 (CSV) tokens from an instance of CsvTokenizer . Inheritance Object CsvReaderVisitorBase Inherited Members Object.Equals(Object) Object.Equals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Object.ReferenceEquals(Object, Object) Object.ToString() Namespace : Cursively Assembly : Cursively.dll Syntax public abstract class CsvReaderVisitorBase Remarks Remarks on the documentation of individual abstract methods indicate when the tokenizer is legally allowed to call that method. Fields | Improve this Doc View Source Null An implementation of CsvReaderVisitorBase that does nothing when it sees any of the tokens. Declaration public static readonly CsvReaderVisitorBase Null Field Value Type Description CsvReaderVisitorBase Methods | Improve this Doc View Source VisitEndOfField(ReadOnlySpan<Byte>) Visits the last part of a field's data. Declaration public abstract void VisitEndOfField(ReadOnlySpan<byte> chunk) Parameters Type Name Description ReadOnlySpan < Byte > chunk The data from the last part of the field. Remarks This method may be called at any time. Any method, including this one, may be called directly after a call to this method. This method may be called without a preceding VisitPartialFieldContents(ReadOnlySpan<Byte>) call, if the field's entire data is contained within the given chunk. | Improve this Doc View Source VisitEndOfRecord() Notifies that all fields in the current record have been visited. Declaration public abstract void VisitEndOfRecord() Remarks This method may only be called as the very next method that gets called after a call to VisitEndOfField(ReadOnlySpan<Byte>) . Only VisitPartialFieldContents(ReadOnlySpan<Byte>) and VisitEndOfField(ReadOnlySpan<Byte>) may be called directly after a call to this method. | Improve this Doc View Source VisitPartialFieldContents(ReadOnlySpan<Byte>) Visits part of a field's data. Declaration public abstract void VisitPartialFieldContents(ReadOnlySpan<byte> chunk) Parameters Type Name Description ReadOnlySpan < Byte > chunk The data from this part of the field. Remarks This method may be called at any time. Only VisitPartialFieldContents(ReadOnlySpan<Byte>) and VisitEndOfField(ReadOnlySpan<Byte>) may be called directly after a call to this method. There are multiple reasons why this method may be called instead of going straight to calling VisitEndOfField(ReadOnlySpan<Byte>) : Field is split across multiple read buffer chunks, or else it runs up to the very end of a read buffer chunk, but we can't prove it without the first byte of the next chunk or a ProcessEndOfStream(CsvReaderVisitorBase) call. Quoted field contains a literal quote that was escaped in the original stream, and so we cannot yield the entire field data as-is. Stream does not conform to RFC 4180, and optimizing such streams to avoid this case."
  }
}